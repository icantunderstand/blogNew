---
title: AI探秘-LangChain使用不完全指南
date: "2025-10-28"
tags: ["AI"]
---

# LangChain使用不完全指南：从入门到精通

LangChain是一个强大的框架，用于构建基于大语言模型(LLM)的应用程序。它提供了丰富的工具和组件，让开发者能够轻松构建复杂的AI应用。本文将带你从基础概念开始，逐步深入LangChain的各个核心功能。

## 目录

1. [什么是LangChain](#什么是langchain)
2. [LangChain包分类](#langchain包分类)
3. [环境搭建](#环境搭建)
4. [核心组件](#核心组件)
   - [Agents 代理](#1-agents---代理)
   - [Models 模型](#2-models---模型)
   - [Messages 消息](#3-messages---消息)
   - [Middleware 中间件](#4-middleware---中间件)

## 什么是LangChain

LangChain是一个开源框架，专门用于开发由语言模型驱动的应用程序。它提供了：

- **模块化设计**：将复杂的AI应用分解为可重用的组件
- **链式调用**：将多个组件串联起来形成工作流
- **多模型支持**：支持OpenAI、Anthropic、本地模型等多种LLM
- **丰富的工具集**：包含文档处理、向量数据库、记忆系统等

## LangChain包分类

### 1. 核心包（Core Packages）
- `langchain` - 核心框架
- `langchain-core` - 核心组件

### 2. 合作伙伴包（Partner Packages）
这些是由 LangChain 官方合作伙伴维护的包，通常有官方支持和保证：
- `langchain-openai` - OpenAI 集成
- `langchain-anthropic` - Anthropic 集成

### 3. 社区集成包（Community Integrations）
这些是由社区开发和维护的包：
- `langchain-ollama` - Ollama 集成
- `langchain-community` - 社区贡献的集成

## 环境搭建

```bash
# 安装核心包
pip install langchain
pip install langchain-core
# 这里因为要使用ollama本地模型 所以安装langchain-ollama
pip install langchain-ollama
# 安装社区包（包含更多集成）
pip install langchain-community

```
### 基本使用

<CollapsibleCode title="基础LLM调用示例" language="python">
```python
from langchain_ollama import OllamaLLM
llm = OllamaLLM(model="deepseek-r1:1.5b")
response = llm.invoke("你好，请介绍一下你自己")
print(response)
```
</CollapsibleCode>

## 核心组件

### 1. Agents - 代理
Agents将语言模型与工具结合，构建出能够对任务进行推理、决定使用哪些工具，并以迭代方式推进求解的系统。

<CollapsibleCode title="创建智能代理" language="python">
```python
from langchain_ollama import ChatOllama
from langchain.agents import create_agent
# 本地模型（Ollama）
llm = ChatOllama(model="qwen3:1.7b")

# 创建 Agent
agent = create_agent(
    llm,
)
# 发起对话
result = agent.invoke(
    {"messages": [{"role": "user", "content": "你好呀 你是谁"}]}, 
)

# 展示最终回复
print(result["messages"][-1].content)

```
</CollapsibleCode>


### 2. Models - 模型

模型可以通过两种方式使用：

**1. 与代理配合使用** - 在创建代理时可以动态指定模型
**2. 独立使用** - 模型可以直接调用（在代理循环之外），用于文本生成、分类或提取等任务，无需代理框架

LangChain提供了标准化的模型接口，支持多种LLM提供商：

<CollapsibleCode title="多模型支持示例" language="python">
```python
# 支持多种模型提供商
from langchain_ollama import OllamaLLM
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# 本地模型
local_llm = OllamaLLM(model="deepseek-r1:1.5b")
# OpenAI模型  调用其他模型的时候  需要指定特定模型的API KEY
#  openai_llm = ChatOpenAI(model="gpt-3.5-turbo")
# Anthropic模型
# anthropic_llm = ChatAnthropic(model="claude-3-sonnet")

# 统一的调用接口
def call_model(llm, prompt):
    return llm.invoke(prompt)

response1 = call_model(local_llm, "你好")
```
</CollapsibleCode>

#### 模型调用方法

| 方法 | 功能 | 使用场景 | 返回值 |
|------|------|----------|--------|
| `invoke()` | 同步调用模型 | 单次请求，需要等待结果 | 直接返回完整结果 |
| `stream()` | 流式调用模型 | 实时显示生成过程 | 返回生成器，逐块返回内容 |
| `batch()` | 批量异步处理 | 并行处理多个请求 | 返回所有结果列表 |
| `batch_as_completed()` | 批量异步处理（完成即返回） | 无需等待所有完成 | 完成一个返回一个 |

#### 工具调用
通过绑定工具在大模型会返回请求调用工具的指令,然后根据指令调用工具函数并返回结果.在agent模式中会自动处理这个调用过程

<CollapsibleCode title="工具调用示例" language="python">
```python
from langchain_ollama import ChatOllama
from langchain.tools import tool
# 定义工具函数
@tool
def get_weather(location: str) -> str:
    # 函数的文档字符串
    """Get the weather at a location."""
    return f"It's bad weather in {location}."
# 使用 ChatOllama 而不是 OllamaLLM（ChatOllama 支持工具绑定）
local_llm = ChatOllama(model="qwen3:1.7b")
model_with_tools = local_llm.bind_tools([get_weather]) 
messages = [{"role": "user", "content": "What's the weather in Boston?"}]
ai_msg = model_with_tools.invoke(messages)
messages.append(ai_msg)

#工具注册
tool_registry = {
    "get_weather": get_weather,
}
# 遍历工具调用
for tool_call in ai_msg.tool_calls:
    tool = tool_registry.get(tool_call['name'])
    if not tool:
        continue
    # 调用工具函数
    tool_result = tool.invoke(tool_call['args'])
    # 将工具返回的结果添加到消息列表中
    messages.append(tool_result)
# 最终响应
final_response = model_with_tools.invoke(messages)
print(final_response.content)
```
</CollapsibleCode>

#### 工具调用流程示意

```text
用户输入
   |
   v
LLM 接收 messages 并生成响应（可能包含 tool_calls）
   |
   |-- 若无 tool_calls -------------------------------> 直接返回最终回答
   |
   |-- 若包含 tool_calls:
   |       |
   |       v
   |   解析 tool_calls 列表（name, args）
   |       |
   |       v
   |   查找并调用对应工具（tool_registry[name].invoke(args)）
   |       |
   |       v
   |   将工具返回结果封装为 ToolMessage，追加到 messages
   |       |
   |       v
   |   将扩展后的 messages 再次发送给 LLM（model_with_tools.invoke）
   |       |
   |       v
   |   生成结合工具结果的最终回答
   |
   v
返回给用户
```
#### 结构化输出 

可以通过Model实现格式化处理,可以实现在后续的环节更好的处理
<CollapsibleCode title="结构化输出示例" language="python">
```python
from langchain_ollama import ChatOllama
from pydantic import BaseModel, Field
# 使用 ChatOllama 而不是 OllamaLLM（ChatOllama 支持工具绑定）
local_llm = ChatOllama(model="qwen3:1.7b")

class Movie(BaseModel):
    """A movie with details."""
    title: str = Field(..., description="The title of the movie")
    year: int = Field(..., description="The year the movie was released")
    director: str = Field(..., description="The director of the movie")
    rating: float = Field(..., description="The movie's rating out of 10")
# 使用结构化输出
model_with_structure = local_llm.with_structured_output(Movie)
response = model_with_structure.invoke("Provide details about the movie Inception")
print(response) 
```
</CollapsibleCode>

### 3. Messages - 消息
消息是 LangChain 中模型上下文的基本单元，代表模型的输入与输出。Message 通常包含角色（role）、内容（content）以及元数据（metadata）；

<CollapsibleCode title="消息格式示例" language="python">
```python
from langchain_ollama import ChatOllama
from langchain.messages import HumanMessage, AIMessage, SystemMessage
local_llm = ChatOllama(model="qwen3:1.7b")
# 创建不同类型的消息
# SystemMessage：系统/角色指令，用于设定模型的全局行为与语气（通常放在最前面）
system_msg = SystemMessage(content="你是一个有用的AI助手")
# HumanMessage：用户输入，表示人类的提问或请求
human_msg = HumanMessage(content="请介绍一下Python")
# AIMessage：模型输出，表示上一次 AI 的回答（可用于提供上下文历史）
ai_msg = AIMessage(content="Python是一种高级编程语言...")
# 消息列表（通常顺序是：System → 交替的 Human/AI 历史 → 当前 Human）
messages = [system_msg, human_msg, ai_msg]
# 消息列表
messages = [system_msg, human_msg, ai_msg]
# 使用消息进行对话
response = local_llm.invoke(messages)
print(response.content) 
```
</CollapsibleCode>

#### Message类型

| 类型 | 作用 | 典型内容 | 备注 |
|------|------|----------|------|
| System message | 告诉模型如何行为，并为交互提供全局上下文 | 规则、角色设定、风格要求 | 通常放在最前面，影响整轮对话走向 |
| Human message | 代表用户输入与模型的交互 | 问题、指令、数据 | 也可包含文件摘要、外部信息等 |
| AI message | 模型生成的响应 | 文字内容；可能包含工具调用信息（tool_calls）及元数据 | 当触发工具调用时，会携带调用的工具名与参数 |
| Tool message | 工具调用的输出 | 工具执行结果，如检索内容、计算结果、API响应 | 通常作为后续消息再次提供给模型以生成最终回答 |


### 4. Middleware - 中间件

用于处理请求和响应的中间件：

<CollapsibleCode title="中间件示例" language="python">
```python
from langchain_core.runnables import RunnableLambda
from langchain_ollama import ChatOllama

# 定义中间件
def log_requests(inputs):
    print(f"收到请求: {inputs}")
    return inputs

def log_responses(outputs):
    print(f"返回响应: {outputs}")
    return outputs

# 创建带中间件的链
llm = ChatOllama(model="qwen3:1.7b")

# 添加中间件
chain = (
    RunnableLambda(log_requests) 
    | llm 
    | RunnableLambda(log_responses)
)

# 使用链
response = chain.invoke("你好")
```
</CollapsibleCode>


## 延伸阅读

- [LangChain官方文档](https://python.langchain.com/)
- [LangChain GitHub仓库](https://github.com/langchain-ai/langchain)


