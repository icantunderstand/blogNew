---
title: AI探秘-理解GPT模型架构
date: "2025-09-24"
tags: ["AI"]
---

在本文中主要从GPT模型架构的子模块功能介绍开始,逐步串联整个GPT模型的架构.


## 层归一化

### 层归一化解决的问题
梯度消失/梯度爆炸问题会导致训练过程不稳定,通过层归一化可以提高训练的稳定性和效率

### 层归一化的位置
层归一化通常采用 Pre-LN:分别位于多头注意力和前馈层之前，且在最终输出前有一层。它对每个 token 的特征维做归一化，稳定上下文化后的表示与训练过程。






层归一化: 以提高神经网络训练的稳定性和效率。层归一化的主要思想是调
整神经网络层的激活（输出），使其均值为 0 且方差（单位方差）为 1。这种调整有助于加速权
重的有效收敛，并确保训练过程的一致性和可靠性

激活函数: GELU  
线性层就是对每个位置的隐藏向量做一次仿射变换 y = xW + b，不跨时间维，只在特征维上混合特征。

快捷连接(跳跃连接/残差连接) 



生成文本过程：

在每一步中，模型输出一个矩阵，其中的向量表示有可能的下一个词元。将与下一个词元对
应的向量提取出来，并通过 softmax 函数转换为概率分布。在包含这些概率分数的向量中，找到
最高值的索引，这个索引对应于词元 ID。然后将这个词元 ID 解码为文本，生成序列中的下一个
词元。最后，将这个词元附加到之前的输入中，形成新的输入序列，供下一次迭代使用。这个逐
步的过程使得模型能够按顺序生成文本，从最初的输入上下文中构建连贯的短语和句子





